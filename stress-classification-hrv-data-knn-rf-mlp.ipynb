{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#plotting\nimport matplotlib.pyplot as plt\n#shallowclassifier\nfrom sklearn import datasets, svm, metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve, auc\n\n#math/data libs\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\n#ml libs\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras. layers import Activation\nfrom keras.layers.core import Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.utils import to_categorical","metadata":{"id":"xrHoFTwCQtGB","execution":{"iopub.status.busy":"2022-02-06T17:49:39.657170Z","iopub.execute_input":"2022-02-06T17:49:39.657749Z","iopub.status.idle":"2022-02-06T17:49:39.665239Z","shell.execute_reply.started":"2022-02-06T17:49:39.657714Z","shell.execute_reply":"2022-02-06T17:49:39.663800Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"###SWELL###\n#trainFile = pd.read_csv('../input/swellwesad-hrv-data/HRV Dataset/combined-swell-classification-hrv-train-dataset.csv').drop(columns='NasaTLX class')\n#testFile = pd.read_csv('../input/swellwesad-hrv-data/HRV Dataset/combined-swell-classification-hrv-test-dataset.csv').drop(columns='NasaTLX class')\n\n\n###WESAD###\n#trainFile = pd.read_csv('../input/swellwesad-hrv-data/HRV Dataset/wesad-classification-hrv-train-dataset.csv').drop(columns=\"SSSQ class\")\n#testFile = pd.read_csv('../input/swellwesad-hrv-data/HRV Dataset/wesad-classification-hrv-test-dataset.csv').drop(columns=\"SSSQ class\")\n\n##PPD##\n\ntrainFile = pd.read_csv('../input/ppd-v1/newtrain.csv').drop(columns=\"sr\")\ntestFile = pd.read_csv('../input/ppd-v1/test.csv').drop(columns=\"sr\")\nvalFile = pd.read_csv('../input/ppd-v1/val.csv').drop(columns=\"sr\")","metadata":{"id":"TmHsAC4jSYI9","execution":{"iopub.status.busy":"2022-02-06T17:56:35.500741Z","iopub.execute_input":"2022-02-06T17:56:35.501208Z","iopub.status.idle":"2022-02-06T17:56:35.538953Z","shell.execute_reply.started":"2022-02-06T17:56:35.501162Z","shell.execute_reply":"2022-02-06T17:56:35.538218Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"trainFile.astype('float64')\ntestFile.astype('float64')\nvalFile.astype('float64')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:56:43.275868Z","iopub.execute_input":"2022-02-06T17:56:43.276489Z","iopub.status.idle":"2022-02-06T17:56:43.328628Z","shell.execute_reply.started":"2022-02-06T17:56:43.276453Z","shell.execute_reply":"2022-02-06T17:56:43.327659Z"},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"    age_range  res  edu  mar_sts  prev_job  prev_incm  now_job  now_incm  \\\n0         1.0  1.0  3.0      0.0       5.0        1.0      0.0       0.0   \n1         0.0  1.0  4.0      0.0       0.0        0.0      0.0       0.0   \n2         1.0  1.0  3.0      0.0       0.0        0.0      0.0       0.0   \n3         2.0  0.0  4.0      0.0       0.0        0.0      0.0       0.0   \n4         1.0  1.0  4.0      0.0       2.0        1.0      0.0       0.0   \n5         1.0  1.0  3.0      0.0       0.0        0.0      0.0       0.0   \n6         1.0  1.0  4.0      0.0       5.0        2.0      0.0       0.0   \n7         3.0  1.0  4.0      0.0       0.0        0.0      0.0       0.0   \n8         0.0  1.0  2.0      0.0       0.0        0.0      0.0       0.0   \n9         3.0  1.0  4.0      0.0       0.0        0.0      0.0       0.0   \n10        1.0  1.0  4.0      0.0       0.0        0.0      0.0       0.0   \n11        1.0  1.0  0.0      0.0       0.0        0.0      0.0       0.0   \n12        1.0  1.0  4.0      0.0       1.0        0.0      1.0       0.0   \n13        2.0  1.0  4.0      0.0       0.0        0.0      0.0       0.0   \n14        1.0  1.0  3.0      0.0       0.0        2.0      0.0       4.0   \n\n    hus_edu  hus_incm  ...  breastfeed  baby_ill  worry_baby  rest_monitor  \\\n0       3.0       1.0  ...         0.0       1.0         1.0           0.0   \n1       4.0       4.0  ...         0.0       1.0         0.0           0.0   \n2       3.0       4.0  ...         0.0       1.0         1.0           1.0   \n3       4.0       4.0  ...         0.0       1.0         1.0           1.0   \n4       4.0       2.0  ...         0.0       1.0         1.0           0.0   \n5       2.0       4.0  ...         1.0       1.0         1.0           0.0   \n6       4.0       3.0  ...         0.0       0.0         1.0           0.0   \n7       4.0       4.0  ...         0.0       1.0         1.0           0.0   \n8       2.0       4.0  ...         1.0       1.0         0.0           0.0   \n9       4.0       4.0  ...         0.0       1.0         0.0           1.0   \n10      4.0       3.0  ...         1.0       0.0         0.0           0.0   \n11      4.0       4.0  ...         0.0       1.0         1.0           0.0   \n12      3.0       2.0  ...         1.0       1.0         1.0           0.0   \n13      3.0       2.0  ...         1.0       1.0         0.0           1.0   \n14      4.0       4.0  ...         1.0       1.0         1.0           1.0   \n\n    rest_sleep  angry_aftr_baby  work_feel  PHQ2(bfr)  PHQ2(drng)  EPDS  \n0          1.0              0.0        2.0        0.0         0.0   2.0  \n1          0.0              1.0        0.0        0.0         0.0   0.0  \n2          1.0              0.0        2.0        0.0         0.0   2.0  \n3          1.0              0.0        0.0        0.0         0.0   2.0  \n4          0.0              0.0        2.0        0.0         1.0   2.0  \n5          0.0              0.0        3.0        0.0         0.0   2.0  \n6          0.0              0.0        2.0        0.0         1.0   2.0  \n7          0.0              1.0        2.0        0.0         0.0   0.0  \n8          0.0              0.0        3.0        0.0         1.0   2.0  \n9          0.0              0.0        3.0        0.0         0.0   1.0  \n10         0.0              0.0        3.0        0.0         0.0   2.0  \n11         0.0              0.0        2.0        0.0         0.0   0.0  \n12         1.0              0.0        3.0        0.0         0.0   2.0  \n13         0.0              0.0        3.0        0.0         0.0   0.0  \n14         0.0              0.0        2.0        0.0         1.0   2.0  \n\n[15 rows x 47 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age_range</th>\n      <th>res</th>\n      <th>edu</th>\n      <th>mar_sts</th>\n      <th>prev_job</th>\n      <th>prev_incm</th>\n      <th>now_job</th>\n      <th>now_incm</th>\n      <th>hus_edu</th>\n      <th>hus_incm</th>\n      <th>...</th>\n      <th>breastfeed</th>\n      <th>baby_ill</th>\n      <th>worry_baby</th>\n      <th>rest_monitor</th>\n      <th>rest_sleep</th>\n      <th>angry_aftr_baby</th>\n      <th>work_feel</th>\n      <th>PHQ2(bfr)</th>\n      <th>PHQ2(drng)</th>\n      <th>EPDS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>15 rows Ã— 47 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"np.where(np.isnan(valFile))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:01:43.631493Z","iopub.execute_input":"2022-02-06T18:01:43.632297Z","iopub.status.idle":"2022-02-06T18:01:43.639998Z","shell.execute_reply.started":"2022-02-06T18:01:43.632250Z","shell.execute_reply":"2022-02-06T18:01:43.639149Z"},"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"(array([9]), array([25]))"},"metadata":{}}]},{"cell_type":"code","source":"np.nan_to_num(valFile)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:01:39.301152Z","iopub.execute_input":"2022-02-06T18:01:39.301793Z","iopub.status.idle":"2022-02-06T18:01:39.322701Z","shell.execute_reply.started":"2022-02-06T18:01:39.301756Z","shell.execute_reply":"2022-02-06T18:01:39.322031Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"array([[ 1.,  1.,  3.,  0.,  5.,  1.,  0.,  0.,  3.,  1.,  0.,  1.,  2.,\n         1.,  0.,  3.,  1.,  1.,  0.,  0.,  1.,  8.,  1.,  0.,  0.,  0.,\n         2.,  5.,  0.,  0.,  0.,  2.,  2.,  0.,  1.,  0.,  0.,  0.,  1.,\n         1.,  0.,  1.,  0.,  2.,  0.,  0.,  2.],\n       [ 0.,  1.,  4.,  0.,  0.,  0.,  0.,  0.,  4.,  4.,  0.,  1.,  0.,\n         0.,  0.,  3.,  0.,  0.,  0.,  0.,  0.,  9.,  0.,  0.,  0.,  0.,\n         1.,  5.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,\n         0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n       [ 1.,  1.,  3.,  0.,  0.,  0.,  0.,  0.,  3.,  4.,  0.,  3.,  0.,\n         0.,  1.,  8.,  1.,  1.,  0.,  0.,  0.,  5.,  2.,  0.,  1.,  1.,\n         3.,  4.,  0.,  0.,  0.,  2.,  0.,  3.,  0.,  1.,  0.,  0.,  1.,\n         1.,  1.,  1.,  0.,  2.,  0.,  0.,  2.],\n       [ 2.,  0.,  4.,  0.,  0.,  0.,  0.,  0.,  4.,  4.,  0.,  1.,  0.,\n         0.,  0.,  3.,  1.,  0.,  0.,  0.,  0.,  5.,  3.,  5.,  1.,  1.,\n         1.,  5.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,\n         1.,  1.,  1.,  0.,  0.,  0.,  0.,  2.],\n       [ 1.,  1.,  4.,  0.,  2.,  1.,  0.,  0.,  4.,  2.,  0.,  1.,  2.,\n         0.,  1., 11.,  2.,  1.,  0.,  0.,  0.,  7.,  2.,  2.,  1.,  0.,\n         1.,  5.,  1.,  0.,  0.,  2.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,\n         1.,  0.,  0.,  0.,  2.,  0.,  1.,  2.],\n       [ 1.,  1.,  3.,  0.,  0.,  0.,  0.,  0.,  2.,  4.,  0.,  2.,  0.,\n         0.,  0.,  4.,  0.,  1.,  0.,  0.,  1.,  2.,  1.,  0.,  0.,  0.,\n         2.,  5.,  0.,  0.,  0.,  0.,  2.,  6.,  0.,  1.,  1.,  1.,  1.,\n         1.,  0.,  0.,  0.,  3.,  0.,  0.,  2.],\n       [ 1.,  1.,  4.,  0.,  5.,  2.,  0.,  0.,  4.,  3.,  0.,  1.,  0.,\n         0.,  0.,  3.,  2.,  0.,  0.,  0.,  1.,  7.,  1.,  1.,  1.,  0.,\n         1.,  5.,  0.,  1.,  0.,  2.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,\n         1.,  0.,  0.,  0.,  2.,  0.,  1.,  2.],\n       [ 3.,  1.,  4.,  0.,  0.,  0.,  0.,  0.,  4.,  4.,  0.,  2.,  0.,\n         0.,  0.,  4.,  1.,  0.,  0.,  0.,  0.,  7.,  0.,  0.,  0.,  0.,\n         2.,  4.,  0.,  0.,  0.,  0.,  0.,  6.,  1.,  1.,  0.,  0.,  1.,\n         1.,  0.,  0.,  1.,  2.,  0.,  0.,  0.],\n       [ 0.,  1.,  2.,  0.,  0.,  0.,  0.,  0.,  2.,  4.,  0.,  1.,  0.,\n         0.,  0.,  3.,  2.,  0.,  0.,  0.,  0.,  9.,  0.,  0.,  1.,  0.,\n         1.,  5.,  1.,  0.,  0.,  0.,  3.,  0.,  0.,  1.,  1.,  1.,  1.,\n         0.,  0.,  0.,  0.,  3.,  0.,  1.,  2.],\n       [ 3.,  1.,  4.,  0.,  0.,  0.,  0.,  0.,  4.,  4.,  0.,  1.,  0.,\n         1.,  0.,  3.,  1.,  0.,  0.,  0.,  0.,  9.,  0.,  0.,  0.,  0.,\n         2.,  5.,  0.,  0.,  1.,  2.,  3.,  0.,  0.,  1.,  1.,  0.,  1.,\n         0.,  1.,  0.,  0.,  3.,  0.,  0.,  1.],\n       [ 1.,  1.,  4.,  0.,  0.,  0.,  0.,  0.,  4.,  3.,  0.,  2.,  0.,\n         0.,  0.,  3.,  0.,  0.,  0.,  0.,  0.,  8.,  1.,  0.,  0.,  0.,\n         2.,  4.,  1.,  0.,  0.,  2.,  3.,  4.,  1.,  0.,  0.,  1.,  0.,\n         0.,  0.,  0.,  0.,  3.,  0.,  0.,  2.],\n       [ 1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  4.,  4.,  0.,  2.,  0.,\n         1.,  0.,  4.,  1.,  0.,  0.,  0.,  0.,  6.,  0.,  0.,  0.,  0.,\n         3.,  4.,  0.,  0.,  1.,  2.,  0.,  4.,  0.,  1.,  0.,  0.,  1.,\n         1.,  0.,  0.,  0.,  2.,  0.,  0.,  0.],\n       [ 1.,  1.,  4.,  0.,  1.,  0.,  1.,  0.,  3.,  2.,  0.,  2.,  0.,\n         0.,  1., 10.,  2.,  2.,  0.,  0.,  0.,  3.,  1.,  0.,  1.,  1.,\n         2.,  4.,  1.,  1.,  0.,  1.,  3.,  4.,  1.,  0.,  0.,  1.,  1.,\n         1.,  0.,  1.,  0.,  3.,  0.,  0.,  2.],\n       [ 2.,  1.,  4.,  0.,  0.,  0.,  0.,  0.,  3.,  2.,  0.,  1.,  0.,\n         0.,  1.,  5.,  0.,  0.,  0.,  0.,  0.,  8.,  1.,  5.,  0.,  0.,\n         1.,  4.,  0.,  0.,  0.,  1.,  2.,  0.,  1.,  0.,  1.,  1.,  1.,\n         0.,  1.,  0.,  0.,  3.,  0.,  0.,  0.],\n       [ 1.,  1.,  3.,  0.,  0.,  2.,  0.,  4.,  4.,  4.,  0.,  1.,  1.,\n         1.,  0.,  3.,  1.,  0.,  0.,  0.,  0.,  7.,  1.,  4.,  1.,  0.,\n         2.,  0.,  1.,  0.,  0.,  1.,  3.,  2.,  1.,  0.,  0.,  1.,  1.,\n         1.,  1.,  0.,  0.,  2.,  0.,  1.,  2.]])"},"metadata":{}}]},{"cell_type":"code","source":"#from sklearn.model_selection import train_test_split\n#trainFile, testFile = train_test_split(mainFile, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:44:16.714983Z","iopub.execute_input":"2022-02-06T17:44:16.715431Z","iopub.status.idle":"2022-02-06T17:44:16.723039Z","shell.execute_reply.started":"2022-02-06T17:44:16.715390Z","shell.execute_reply":"2022-02-06T17:44:16.721992Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"#features\n#X_train = trainFile.drop(columns='condition')\n#y_train = trainFile['condition']\n#X_test = testFile.drop(columns='condition')\n#y_test = testFile['condition']\n\nX_train = trainFile.drop(columns='EPDS')\ny_train = trainFile['EPDS']\nX_test = testFile.drop(columns='EPDS')\ny_test = testFile['EPDS']","metadata":{"id":"t3xkR7WAWfVO","execution":{"iopub.status.busy":"2022-02-06T18:01:52.645635Z","iopub.execute_input":"2022-02-06T18:01:52.646080Z","iopub.status.idle":"2022-02-06T18:01:52.658060Z","shell.execute_reply.started":"2022-02-06T18:01:52.646031Z","shell.execute_reply":"2022-02-06T18:01:52.655742Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"#heatmap\n#import seaborn as sns\n#plt.figure(figsize=(12,10))\n#corr = trainFile.corr()\n#sns.heatmap(corr, annot=False, cmap=plt.cm.Reds)\n#plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:30:17.381605Z","iopub.execute_input":"2022-02-06T17:30:17.382125Z","iopub.status.idle":"2022-02-06T17:30:17.385978Z","shell.execute_reply.started":"2022-02-06T17:30:17.382067Z","shell.execute_reply":"2022-02-06T17:30:17.385135Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **KNN** ","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nprint('Accuracy of K-NN classifier on training set: {:.2f}'.format(knn.score(X_train, y_train)))\nprint('Accuracy of K-NN classifier on test set: {:.2f}'.format(knn.score(X_test, y_test)))","metadata":{"id":"INrVmaTjYX1j","outputId":"0cf265a2-ffc9-459b-b809-041ca5e208ed","execution":{"iopub.status.busy":"2022-02-06T18:01:58.416637Z","iopub.execute_input":"2022-02-06T18:01:58.416893Z","iopub.status.idle":"2022-02-06T18:01:58.449859Z","shell.execute_reply.started":"2022-02-06T18:01:58.416861Z","shell.execute_reply":"2022-02-06T18:01:58.449151Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"Accuracy of K-NN classifier on training set: 0.57\nAccuracy of K-NN classifier on test set: 0.53\n","output_type":"stream"}]},{"cell_type":"code","source":"#y_pred = knn.predict(X_test)\n#y_pred[:10]\n#accuracy_score(y_test,y_pred)\n\ny_score = knn.predict(X_test)\ny_score[:30]\naccuracy_score(y_test,y_score)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:02:07.682734Z","iopub.execute_input":"2022-02-06T18:02:07.682985Z","iopub.status.idle":"2022-02-06T18:02:07.696723Z","shell.execute_reply.started":"2022-02-06T18:02:07.682955Z","shell.execute_reply":"2022-02-06T18:02:07.696097Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"0.5333333333333333"},"metadata":{}}]},{"cell_type":"code","source":"print(classification_report(y_test,y_score))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:02:12.273472Z","iopub.execute_input":"2022-02-06T18:02:12.273996Z","iopub.status.idle":"2022-02-06T18:02:12.286101Z","shell.execute_reply.started":"2022-02-06T18:02:12.273958Z","shell.execute_reply":"2022-02-06T18:02:12.285426Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.56      0.64      0.60        14\n           1       0.33      0.33      0.33         3\n           2       0.55      0.46      0.50        13\n\n    accuracy                           0.53        30\n   macro avg       0.48      0.48      0.48        30\nweighted avg       0.53      0.53      0.53        30\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#from sklearn.metrics import confusion_matrix\n#import seaborn as sn\n#import pandas as pd\n#import matplotlib.pyplot as plt\n\n#cm= confusion_matrix(y_test, y_score)\n#print(cm)\n#df_cm = pd.DataFrame(cm, index=[\"low\", \"medium\", \"high\"], columns=[\"low\", \"medium\", \"high\"])\n#plt.figure(figsize = (10,7))\n#sn.heatmap(df_cm, annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:02:15.623153Z","iopub.execute_input":"2022-02-06T18:02:15.623513Z","iopub.status.idle":"2022-02-06T18:02:15.899635Z","shell.execute_reply.started":"2022-02-06T18:02:15.623478Z","shell.execute_reply":"2022-02-06T18:02:15.898970Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"[[9 1 4]\n [1 1 1]\n [6 1 6]]\n","output_type":"stream"},{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x504 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, y_score, pos_label=2)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T11:55:21.129231Z","iopub.execute_input":"2022-02-06T11:55:21.129855Z","iopub.status.idle":"2022-02-06T11:55:21.376427Z","shell.execute_reply.started":"2022-02-06T11:55:21.129813Z","shell.execute_reply":"2022-02-06T11:55:21.375741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **SVM**","metadata":{}},{"cell_type":"code","source":"sv = svm.SVC()\nsv.fit(X_train,y_train)\nprint('Accuracy of SVM classifier on training set: {:.2f}'.format(sv.score(X_train, y_train)))\nprint('Accuracy of SVM classifier on test set: {:.2f}'.format(sv.score(X_test, y_test)))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:02:28.228430Z","iopub.execute_input":"2022-02-06T18:02:28.229179Z","iopub.status.idle":"2022-02-06T18:02:28.260444Z","shell.execute_reply.started":"2022-02-06T18:02:28.229096Z","shell.execute_reply":"2022-02-06T18:02:28.259780Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"Accuracy of SVM classifier on training set: 0.53\nAccuracy of SVM classifier on test set: 0.57\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = sv.predict(X_test)\ny_pred[:10]\naccuracy_score(y_test,y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:02:32.590625Z","iopub.execute_input":"2022-02-06T18:02:32.591244Z","iopub.status.idle":"2022-02-06T18:02:32.602390Z","shell.execute_reply.started":"2022-02-06T18:02:32.591204Z","shell.execute_reply":"2022-02-06T18:02:32.601616Z"},"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"0.5666666666666667"},"metadata":{}}]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:02:35.363481Z","iopub.execute_input":"2022-02-06T18:02:35.364340Z","iopub.status.idle":"2022-02-06T18:02:35.376215Z","shell.execute_reply.started":"2022-02-06T18:02:35.364291Z","shell.execute_reply":"2022-02-06T18:02:35.375455Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.67      0.57      0.62        14\n           1       0.00      0.00      0.00         3\n           2       0.50      0.69      0.58        13\n\n    accuracy                           0.57        30\n   macro avg       0.39      0.42      0.40        30\nweighted avg       0.53      0.57      0.54        30\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Random Forest**","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier()\nrf.fit(X_train,y_train)\nprint('Accuracy of RF classifier on training set: {:.2f}'.format(rf.score(X_train, y_train)))\nprint('Accuracy of RF classifier on test set: {:.2f}'.format(rf.score(X_test, y_test)))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:02:42.488968Z","iopub.execute_input":"2022-02-06T18:02:42.489541Z","iopub.status.idle":"2022-02-06T18:02:42.715762Z","shell.execute_reply.started":"2022-02-06T18:02:42.489502Z","shell.execute_reply":"2022-02-06T18:02:42.714951Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"Accuracy of RF classifier on training set: 1.00\nAccuracy of RF classifier on test set: 0.70\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = rf.predict(X_test)\ny_pred[:10]\naccuracy_score(y_test,y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:02:52.421778Z","iopub.execute_input":"2022-02-06T18:02:52.422035Z","iopub.status.idle":"2022-02-06T18:02:52.443185Z","shell.execute_reply.started":"2022-02-06T18:02:52.422004Z","shell.execute_reply":"2022-02-06T18:02:52.441999Z"},"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"0.7"},"metadata":{}}]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:02:57.467232Z","iopub.execute_input":"2022-02-06T18:02:57.467749Z","iopub.status.idle":"2022-02-06T18:02:57.478350Z","shell.execute_reply.started":"2022-02-06T18:02:57.467713Z","shell.execute_reply":"2022-02-06T18:02:57.477606Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.77      0.71      0.74        14\n           1       0.50      0.33      0.40         3\n           2       0.67      0.77      0.71        13\n\n    accuracy                           0.70        30\n   macro avg       0.65      0.61      0.62        30\nweighted avg       0.70      0.70      0.70        30\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **NN**","metadata":{}},{"cell_type":"code","source":"#train\ntrain_samples = trainFile.drop(columns='EPDS').to_numpy()\ntrain_labels = trainFile['EPDS'].to_numpy()\n\n#test\ntest_samples = testFile.drop(columns='EPDS').to_numpy()\ntest_labels = testFile['EPDS'].to_numpy()\n\n#val\nval_samples = valFile.drop(columns='EPDS').to_numpy()\nval_labels = valFile['EPDS'].to_numpy()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:03:04.753833Z","iopub.execute_input":"2022-02-06T18:03:04.754102Z","iopub.status.idle":"2022-02-06T18:03:04.762964Z","shell.execute_reply.started":"2022-02-06T18:03:04.754068Z","shell.execute_reply":"2022-02-06T18:03:04.762184Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"#normalizing features\nscaler = MinMaxScaler(feature_range=(0,1))\ntrain_samples = scaler.fit_transform(train_samples)\ntest_samples = scaler.fit_transform(test_samples)\nval_samples = scaler.fit_transform(val_samples)\n\n#one-hot-encoding labels\none_hot_encoder = OneHotEncoder(categories='auto')\ntrain_labels = one_hot_encoder.fit_transform(train_labels.reshape(-1, 1)).toarray()\ntest_labels = one_hot_encoder.fit_transform(test_labels.reshape(-1, 1)).toarray()\nval_labels = one_hot_encoder.fit_transform(val_labels.reshape(-1, 1)).toarray()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:03:07.054862Z","iopub.execute_input":"2022-02-06T18:03:07.055129Z","iopub.status.idle":"2022-02-06T18:03:07.065796Z","shell.execute_reply.started":"2022-02-06T18:03:07.055086Z","shell.execute_reply":"2022-02-06T18:03:07.065012Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"a,b= train_samples.shape\nprint(a)\nprint(b)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:03:11.753466Z","iopub.execute_input":"2022-02-06T18:03:11.753711Z","iopub.status.idle":"2022-02-06T18:03:11.759574Z","shell.execute_reply.started":"2022-02-06T18:03:11.753682Z","shell.execute_reply":"2022-02-06T18:03:11.758779Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"305\n46\n","output_type":"stream"}]},{"cell_type":"code","source":"#build the model\nmodel = Sequential([\n    Dense(256, input_shape=[b,], activation='relu'),\n    Dense(128, activation='relu'),\n    Dense(64, activation='relu'),\n    Dense(16, activation='relu'),\n    Dense(3, activation='softmax')\n])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:12:10.683378Z","iopub.execute_input":"2022-02-06T18:12:10.684071Z","iopub.status.idle":"2022-02-06T18:12:10.731607Z","shell.execute_reply.started":"2022-02-06T18:12:10.684031Z","shell.execute_reply":"2022-02-06T18:12:10.730186Z"},"trusted":true},"execution_count":109,"outputs":[{"name":"stdout","text":"Model: \"sequential_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_22 (Dense)             (None, 256)               12032     \n_________________________________________________________________\ndense_23 (Dense)             (None, 128)               32896     \n_________________________________________________________________\ndense_24 (Dense)             (None, 64)                8256      \n_________________________________________________________________\ndense_25 (Dense)             (None, 16)                1040      \n_________________________________________________________________\ndense_26 (Dense)             (None, 3)                 51        \n=================================================================\nTotal params: 54,275\nTrainable params: 54,275\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"checkpoint = keras.callbacks.ModelCheckpoint('/kaggle/working/model-{epoch:03d}-{accuracy:03f}-{val_accuracy:03f}.h5', verbose=1, monitor='val_accuracy',save_best_only=True, mode='max')\n\nmodel.compile(Adam(learning_rate=.0001),loss='categorical_crossentropy',metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:21:31.155260Z","iopub.execute_input":"2022-02-06T18:21:31.155773Z","iopub.status.idle":"2022-02-06T18:21:31.167925Z","shell.execute_reply.started":"2022-02-06T18:21:31.155738Z","shell.execute_reply":"2022-02-06T18:21:31.167272Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"#need gpu to run the code\nhis = model.fit(train_samples, train_labels, validation_data=(val_samples, val_labels), batch_size=64, epochs=100, shuffle=False, verbose=2, callbacks = [checkpoint])\n#model.fit(train_samples, train_labels, validation_data=(val_samples, val_labels), batch_size=10, epochs=500, shuffle=False, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:21:32.910425Z","iopub.execute_input":"2022-02-06T18:21:32.910869Z","iopub.status.idle":"2022-02-06T18:21:36.542098Z","shell.execute_reply.started":"2022-02-06T18:21:32.910829Z","shell.execute_reply":"2022-02-06T18:21:36.541405Z"},"trusted":true},"execution_count":127,"outputs":[{"name":"stdout","text":"Epoch 1/100\n5/5 - 1s - loss: 1.6357e-05 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00001: val_accuracy improved from -inf to 0.66667, saving model to /kaggle/working/model-001-1.000000-0.666667.h5\nEpoch 2/100\n5/5 - 0s - loss: 1.1775e-05 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00002: val_accuracy did not improve from 0.66667\nEpoch 3/100\n5/5 - 0s - loss: 9.4596e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00003: val_accuracy did not improve from 0.66667\nEpoch 4/100\n5/5 - 0s - loss: 7.5734e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00004: val_accuracy did not improve from 0.66667\nEpoch 5/100\n5/5 - 0s - loss: 6.4920e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00005: val_accuracy did not improve from 0.66667\nEpoch 6/100\n5/5 - 0s - loss: 5.5930e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00006: val_accuracy did not improve from 0.66667\nEpoch 7/100\n5/5 - 0s - loss: 4.9341e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00007: val_accuracy did not improve from 0.66667\nEpoch 8/100\n5/5 - 0s - loss: 4.3826e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00008: val_accuracy did not improve from 0.66667\nEpoch 9/100\n5/5 - 0s - loss: 3.9659e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00009: val_accuracy did not improve from 0.66667\nEpoch 10/100\n5/5 - 0s - loss: 3.6216e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00010: val_accuracy did not improve from 0.66667\nEpoch 11/100\n5/5 - 0s - loss: 3.3277e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00011: val_accuracy did not improve from 0.66667\nEpoch 12/100\n5/5 - 0s - loss: 3.0736e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00012: val_accuracy did not improve from 0.66667\nEpoch 13/100\n5/5 - 0s - loss: 2.8579e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00013: val_accuracy did not improve from 0.66667\nEpoch 14/100\n5/5 - 0s - loss: 2.6675e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00014: val_accuracy did not improve from 0.66667\nEpoch 15/100\n5/5 - 0s - loss: 2.5018e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00015: val_accuracy did not improve from 0.66667\nEpoch 16/100\n5/5 - 0s - loss: 2.3568e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00016: val_accuracy did not improve from 0.66667\nEpoch 17/100\n5/5 - 0s - loss: 2.2216e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00017: val_accuracy did not improve from 0.66667\nEpoch 18/100\n5/5 - 0s - loss: 2.1055e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00018: val_accuracy did not improve from 0.66667\nEpoch 19/100\n5/5 - 0s - loss: 1.9972e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00019: val_accuracy did not improve from 0.66667\nEpoch 20/100\n5/5 - 0s - loss: 1.8984e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00020: val_accuracy did not improve from 0.66667\nEpoch 21/100\n5/5 - 0s - loss: 1.8120e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00021: val_accuracy did not improve from 0.66667\nEpoch 22/100\n5/5 - 0s - loss: 1.7256e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00022: val_accuracy did not improve from 0.66667\nEpoch 23/100\n5/5 - 0s - loss: 1.6506e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00023: val_accuracy did not improve from 0.66667\nEpoch 24/100\n5/5 - 0s - loss: 1.5802e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00024: val_accuracy did not improve from 0.66667\nEpoch 25/100\n5/5 - 0s - loss: 1.5181e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00025: val_accuracy did not improve from 0.66667\nEpoch 26/100\n5/5 - 0s - loss: 1.4547e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00026: val_accuracy did not improve from 0.66667\nEpoch 27/100\n5/5 - 0s - loss: 1.3989e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00027: val_accuracy did not improve from 0.66667\nEpoch 28/100\n5/5 - 0s - loss: 1.3441e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00028: val_accuracy did not improve from 0.66667\nEpoch 29/100\n5/5 - 0s - loss: 1.2984e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00029: val_accuracy did not improve from 0.66667\nEpoch 30/100\n5/5 - 0s - loss: 1.2527e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00030: val_accuracy did not improve from 0.66667\nEpoch 31/100\n5/5 - 0s - loss: 1.2077e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00031: val_accuracy did not improve from 0.66667\nEpoch 32/100\n5/5 - 0s - loss: 1.1671e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00032: val_accuracy did not improve from 0.66667\nEpoch 33/100\n5/5 - 0s - loss: 1.1292e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00033: val_accuracy did not improve from 0.66667\nEpoch 34/100\n5/5 - 0s - loss: 1.0909e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00034: val_accuracy did not improve from 0.66667\nEpoch 35/100\n5/5 - 0s - loss: 1.0592e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00035: val_accuracy did not improve from 0.66667\nEpoch 36/100\n5/5 - 0s - loss: 1.0252e-06 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00036: val_accuracy did not improve from 0.66667\nEpoch 37/100\n5/5 - 0s - loss: 9.9354e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00037: val_accuracy did not improve from 0.66667\nEpoch 38/100\n5/5 - 0s - loss: 9.6149e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00038: val_accuracy did not improve from 0.66667\nEpoch 39/100\n5/5 - 0s - loss: 9.3413e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00039: val_accuracy did not improve from 0.66667\nEpoch 40/100\n5/5 - 0s - loss: 9.1146e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00040: val_accuracy did not improve from 0.66667\nEpoch 41/100\n5/5 - 0s - loss: 8.8488e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00041: val_accuracy did not improve from 0.66667\nEpoch 42/100\n5/5 - 0s - loss: 8.6143e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00042: val_accuracy did not improve from 0.66667\nEpoch 43/100\n5/5 - 0s - loss: 8.3720e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00043: val_accuracy did not improve from 0.66667\nEpoch 44/100\n5/5 - 0s - loss: 8.1805e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00044: val_accuracy did not improve from 0.66667\nEpoch 45/100\n5/5 - 0s - loss: 7.9616e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00045: val_accuracy did not improve from 0.66667\nEpoch 46/100\n5/5 - 0s - loss: 7.7427e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00046: val_accuracy did not improve from 0.66667\nEpoch 47/100\n5/5 - 0s - loss: 7.5200e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00047: val_accuracy did not improve from 0.66667\nEpoch 48/100\n5/5 - 0s - loss: 7.3519e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00048: val_accuracy did not improve from 0.66667\nEpoch 49/100\n5/5 - 0s - loss: 7.1877e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00049: val_accuracy did not improve from 0.66667\nEpoch 50/100\n5/5 - 0s - loss: 6.9610e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00050: val_accuracy did not improve from 0.66667\nEpoch 51/100\n5/5 - 0s - loss: 6.8203e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00051: val_accuracy did not improve from 0.66667\nEpoch 52/100\n5/5 - 0s - loss: 6.6327e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00052: val_accuracy did not improve from 0.66667\nEpoch 53/100\n5/5 - 0s - loss: 6.4959e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00053: val_accuracy did not improve from 0.66667\nEpoch 54/100\n5/5 - 0s - loss: 6.3591e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00054: val_accuracy did not improve from 0.66667\nEpoch 55/100\n5/5 - 0s - loss: 6.2028e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00055: val_accuracy did not improve from 0.66667\nEpoch 56/100\n5/5 - 0s - loss: 6.0347e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00056: val_accuracy did not improve from 0.66667\nEpoch 57/100\n5/5 - 0s - loss: 5.9136e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00057: val_accuracy did not improve from 0.66667\nEpoch 58/100\n5/5 - 0s - loss: 5.8080e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00058: val_accuracy did not improve from 0.66667\nEpoch 59/100\n5/5 - 0s - loss: 5.6908e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00059: val_accuracy did not improve from 0.66667\nEpoch 60/100\n5/5 - 0s - loss: 5.5774e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00060: val_accuracy did not improve from 0.66667\nEpoch 61/100\n5/5 - 0s - loss: 5.4563e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00061: val_accuracy did not improve from 0.66667\nEpoch 62/100\n5/5 - 0s - loss: 5.3351e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00062: val_accuracy did not improve from 0.66667\nEpoch 63/100\n5/5 - 0s - loss: 5.2335e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00063: val_accuracy did not improve from 0.66667\nEpoch 64/100\n5/5 - 0s - loss: 5.1436e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00064: val_accuracy did not improve from 0.66667\nEpoch 65/100\n5/5 - 0s - loss: 5.0420e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00065: val_accuracy did not improve from 0.66667\nEpoch 66/100\n5/5 - 0s - loss: 4.9677e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00066: val_accuracy did not improve from 0.66667\nEpoch 67/100\n5/5 - 0s - loss: 4.8778e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00067: val_accuracy did not improve from 0.66667\nEpoch 68/100\n5/5 - 0s - loss: 4.7879e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00068: val_accuracy did not improve from 0.66667\nEpoch 69/100\n5/5 - 0s - loss: 4.7176e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00069: val_accuracy did not improve from 0.66667\nEpoch 70/100\n5/5 - 0s - loss: 4.6667e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00070: val_accuracy did not improve from 0.66667\nEpoch 71/100\n5/5 - 0s - loss: 4.5769e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00071: val_accuracy did not improve from 0.66667\nEpoch 72/100\n5/5 - 0s - loss: 4.4713e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00072: val_accuracy did not improve from 0.66667\nEpoch 73/100\n5/5 - 0s - loss: 4.3658e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00073: val_accuracy did not improve from 0.66667\nEpoch 74/100\n5/5 - 0s - loss: 4.2681e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00074: val_accuracy did not improve from 0.66667\nEpoch 75/100\n5/5 - 0s - loss: 4.1586e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00075: val_accuracy did not improve from 0.66667\nEpoch 76/100\n5/5 - 0s - loss: 4.1000e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00076: val_accuracy did not improve from 0.66667\nEpoch 77/100\n5/5 - 0s - loss: 4.0101e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00077: val_accuracy did not improve from 0.66667\nEpoch 78/100\n5/5 - 0s - loss: 3.9476e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00078: val_accuracy did not improve from 0.66667\nEpoch 79/100\n5/5 - 0s - loss: 3.8811e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00079: val_accuracy did not improve from 0.66667\nEpoch 80/100\n5/5 - 0s - loss: 3.8264e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00080: val_accuracy did not improve from 0.66667\nEpoch 81/100\n5/5 - 0s - loss: 3.7834e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00081: val_accuracy did not improve from 0.66667\nEpoch 82/100\n5/5 - 0s - loss: 3.7326e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00082: val_accuracy did not improve from 0.66667\nEpoch 83/100\n5/5 - 0s - loss: 3.7013e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00083: val_accuracy did not improve from 0.66667\nEpoch 84/100\n5/5 - 0s - loss: 3.6193e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00084: val_accuracy did not improve from 0.66667\nEpoch 85/100\n5/5 - 0s - loss: 3.5606e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00085: val_accuracy did not improve from 0.66667\nEpoch 86/100\n5/5 - 0s - loss: 3.5020e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00086: val_accuracy did not improve from 0.66667\nEpoch 87/100\n5/5 - 0s - loss: 3.4395e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00087: val_accuracy did not improve from 0.66667\nEpoch 88/100\n5/5 - 0s - loss: 3.4082e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00088: val_accuracy did not improve from 0.66667\nEpoch 89/100\n5/5 - 0s - loss: 3.3496e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00089: val_accuracy did not improve from 0.66667\nEpoch 90/100\n5/5 - 0s - loss: 3.2714e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00090: val_accuracy did not improve from 0.66667\nEpoch 91/100\n5/5 - 0s - loss: 3.2519e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00091: val_accuracy did not improve from 0.66667\nEpoch 92/100\n5/5 - 0s - loss: 3.1932e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00092: val_accuracy did not improve from 0.66667\nEpoch 93/100\n5/5 - 0s - loss: 3.1542e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00093: val_accuracy did not improve from 0.66667\nEpoch 94/100\n5/5 - 0s - loss: 3.1073e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00094: val_accuracy did not improve from 0.66667\nEpoch 95/100\n5/5 - 0s - loss: 3.0564e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00095: val_accuracy did not improve from 0.66667\nEpoch 96/100\n5/5 - 0s - loss: 2.9978e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00096: val_accuracy did not improve from 0.66667\nEpoch 97/100\n5/5 - 0s - loss: 2.9236e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00097: val_accuracy did not improve from 0.66667\nEpoch 98/100\n5/5 - 0s - loss: 2.9001e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00098: val_accuracy did not improve from 0.66667\nEpoch 99/100\n5/5 - 0s - loss: 2.8610e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00099: val_accuracy did not improve from 0.66667\nEpoch 100/100\n5/5 - 0s - loss: 2.8219e-07 - accuracy: 1.0000 - val_loss: nan - val_accuracy: 0.6667\n\nEpoch 00100: val_accuracy did not improve from 0.66667\n","output_type":"stream"}]},{"cell_type":"code","source":"model.predict(test_samples)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:20:13.053916Z","iopub.execute_input":"2022-02-06T18:20:13.054191Z","iopub.status.idle":"2022-02-06T18:20:13.154012Z","shell.execute_reply.started":"2022-02-06T18:20:13.054157Z","shell.execute_reply":"2022-02-06T18:20:13.153350Z"},"trusted":true},"execution_count":124,"outputs":[{"execution_count":124,"output_type":"execute_result","data":{"text/plain":"array([[1.6833740e-01, 8.2112780e-07, 8.3166176e-01],\n       [5.6438360e-10, 2.3078693e-03, 9.9769211e-01],\n       [2.7175215e-10, 2.1788949e-08, 1.0000000e+00],\n       [6.6320254e-03, 9.9136412e-01, 2.0038311e-03],\n       [8.9348215e-01, 9.9688135e-02, 6.8297130e-03],\n       [3.4308052e-09, 1.9184272e-09, 1.0000000e+00],\n       [6.3345418e-12, 3.6840881e-09, 1.0000000e+00],\n       [9.9997246e-01, 2.1525596e-08, 2.7541626e-05],\n       [9.9112773e-01, 8.8690044e-03, 3.2224907e-06],\n       [3.4367961e-10, 2.0256949e-09, 1.0000000e+00],\n       [3.8406359e-08, 8.3505981e-08, 9.9999988e-01],\n       [9.9217892e-01, 7.8040985e-03, 1.6908631e-05],\n       [9.9975461e-01, 2.6718176e-06, 2.4273111e-04],\n       [9.9999928e-01, 1.7710494e-08, 6.7740990e-07],\n       [4.7470770e-08, 4.9213540e-08, 9.9999988e-01],\n       [9.5977426e-02, 8.5251492e-01, 5.1507700e-02],\n       [1.9112563e-01, 3.5039682e-04, 8.0852401e-01],\n       [9.9999988e-01, 9.0623917e-09, 6.4660618e-08],\n       [3.0333349e-06, 1.0796410e-01, 8.9203292e-01],\n       [5.2933246e-01, 8.7192636e-03, 4.6194822e-01],\n       [1.2386033e-02, 9.7698299e-04, 9.8663700e-01],\n       [9.9455303e-01, 2.2660487e-04, 5.2204309e-03],\n       [5.0778771e-03, 2.2737009e-02, 9.7218508e-01],\n       [4.4858556e-05, 4.6948835e-06, 9.9995041e-01],\n       [2.9209491e-07, 5.4631757e-05, 9.9994504e-01],\n       [4.7216219e-13, 3.1849306e-06, 9.9999678e-01],\n       [9.7515488e-01, 2.6063885e-06, 2.4842614e-02],\n       [9.9999988e-01, 1.2628504e-08, 1.6596887e-07],\n       [9.9664450e-01, 1.3168482e-08, 3.3555767e-03],\n       [9.9972659e-01, 2.5926900e-06, 2.7082191e-04]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"loss, acc = model.evaluate(test_samples, test_labels, verbose=1) \nprint(acc*100)\nprint(loss)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:21:46.302520Z","iopub.execute_input":"2022-02-06T18:21:46.303026Z","iopub.status.idle":"2022-02-06T18:21:46.358633Z","shell.execute_reply.started":"2022-02-06T18:21:46.302988Z","shell.execute_reply":"2022-02-06T18:21:46.357516Z"},"trusted":true},"execution_count":128,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 17ms/step - loss: 3.3867 - accuracy: 0.5667\n56.66666626930237\n3.386723756790161\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save('ppd_nn_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}